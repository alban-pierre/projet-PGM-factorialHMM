\documentclass[10pt,a4paper]{report}
\usepackage[latin1]{inputenc}
%\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=1.5cm,right=1.5cm,top=2.5cm,bottom=2.5cm]{geometry}

\pagenumbering{gobble}

\DeclareMathOperator{\ind}{\perp \!\!\! \perp}



% Le document est sur 6 pages organisées comme ci-dessous :
% 1 3 5
% 2 4 6

% Tu as accès à une imprimante (facilement) ?
\begin{document}

\begin{center}
\resizebox{\linewidth}{!}{\itshape \textbf{Generalization and}}
\end{center}
\vspace{80pt}

\begin{center}
\Huge{\textit{Introduction}}
\end{center}
\vspace{20pt}

\Large{
	
	Hidden Markov Models (HMMs) are widely used for probabilistic models of time series data. Here we used a generalization of HMMs called factorial hidden Markov model, in which a state $S_t$ corresponding to an hidden variable is now represented by a collection of states $S_t = \left(S_t^{(1)},\dots,S_t^{(M)}\right)$ each of which can take $K^{(m)}$ values (figure (a)). Here we consider the case where $K = K^{(m)}$ for all $m$. Furthermore, we assume that $\{(S_t^{(m)})_{t=0}^T\}_{m=0}^M$ are independent Markov chains and that the observation $y_t$ are Gaussian random vector whose mean is a linear function of the hidden variables.
	\newline
	\newline
	
	}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{fHMM.png}
	%\centerline{Factorial Hidden Markov Model}
	\label{fig:b}
\end{figure}


\newpage
\begin{center}
\Huge{\textit{Inference and learning}}
\end{center}
\vspace{20pt}

\Large{

Here we want to learn the parameters of a factorial HMM for a given structure. For that we use the EM algorithm to estimate them. While the M step is simple and tractable, the E step is computationally more difficult. We have four options:

\begin{itemize}
	\item[-]Exact inference: it is computationally intractable for a big model.
	\item[-]Inference using Gibbs sampling.
	\item[-]Completely factorized variational inference: we approximate the posterior distribution by a tractable distribution and we make the assumption that all the hidden variables are independent.
	\item[-]Structured variational inference: same as above but we take into account the structure of the hidden variables (figure(b)).
\end{itemize}

}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{sva.png}
	%\centerline{Factorial Hidden Markov Model}
	\label{fig:b}
\end{figure}








\newpage
\begin{center}
\resizebox{\linewidth}{!}{\itshape \textbf{acceleration of HMMs}}
\end{center}
\vspace{5pt}
\begin{center}
	\LARGE{Matthieu Jedor \& Alban Pierre}
	\vspace{15pt}
	
	\Large{Keywords: Hidden Markov models, time series, EM algorithm, graphical models, Bayesian networks, mean field
theory}
\end{center}
\vspace{30pt}
\begin{center}
\Huge{\textit{Experiments on synthetic data}}
\end{center}
\vspace{20pt}
\Large{
	We generate synthetic datasets using a factorial hidden Markov model and then we tried to learn the parameters with our 4 algorithms. Figure 1 shows the log-likelihood accross iterations and figure 2 shows the results in terms of centers found.
	
}

TODO add log-likelihood plots and time of computation, and the protocol of experimentations

\newpage
-










\newpage
\begin{center}
\resizebox{\linewidth}{!}{\itshape \textbf{using factorial HMMs}}
\end{center}
\vspace{80pt}
\begin{center}
\Huge{\textit{Experiments on real data}}
\end{center}

\newpage
-





\iffalse

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{P2.png}
	\centerline{Paramètres utilisés pour calculer les probabilités des états cachés}
	\label{fig:b}
\end{figure}


\begin{tabular}{lcc}
	& Entrainement & Test \\
	EM mixture de gaussiennes & -2371 & -2453 \\
	EM chaine de Markov cachée & -1899 & -1957 \\
\end{tabular}
\newline

\fi

\end{document}
